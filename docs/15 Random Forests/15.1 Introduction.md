# 导言

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-01-28                               |
| 更新 | 2018-01-02|

8.7节中的bagging或者bootstrap aggregation是为了降低估计的预测函数方差的技巧。bagging似乎对于高方差、低偏差的过程表现得特别好，比如树。对于回归， 我们简单地对训练数据的bootstrap版本的进行多次同样的回归树，然后取结果的平均。对于分类，作为committee的每棵树对预测的类别进行投票。

!!! note "weiya注"
    在8.8节，用了这样一句话来介绍Committee Method，“Committee methods take a simple unweighted average of the predictions from each model, essentially giving equal probability to each model.” 简单来说就是，对于一系列模型，committee方法将这一系列模型得到的预测值进行简单平均。比如，对于分类而言，模型的预测值是类别标签，Committee方法则是统计各个模型预测的类别标签（然后进行平均，如果只关注票数，是否平均并不重要），这就类似选举时委员会投票。

第10章中的boosting一开始也是作为committee的方法提出来的，尽管不像bagging，但是作为committee的weak learners随着时间不断进化，并且成员会投出带有权重的票。boosting似乎在大部分问题上表现最好，而且成为更好的选择。

随机森林(Breiman, 2011[^1])是对bagging本质上的修改，而且建了一个去相关性(de-correlated)树的集合，然后进行平均。在许多问题上，随机森林的表现与boosting的表现很相似，而且更加简单地进行训练和调参。结果导致，随机森林很流行，而且可以通过各种包来实现。

[^1]: Breiman, L. (2001). Random forests, Machine Learning 45: 5–32.
