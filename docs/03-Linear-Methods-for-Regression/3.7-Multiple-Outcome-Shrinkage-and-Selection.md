# 3.7 多重输出的收缩和选择

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=103) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 更新 | {{ git_revision_date }} |
| 状态 | Done|

如 [3.2.4 节](3.2-Linear-Regression-Models-and-Least-Squares/index.html#_3)所述，多重输出线性模型的最小二乘估计可以简单地看成是关于每个输出的最小二乘估计．

!!! note "weiya 注："
    $$
    \hat{\beta}^{ridge}=(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\tag{3.44}\label{3.44}
    $$

为了在多重输出情形中应用选择和收缩的方法，可以对每个输出变量单独地应用单变量的技巧，或对全部的输出变量同时进行．举个例子，对于岭回归，我们可以对输出矩阵 $Y$ 的 $K$ 列中的每一列应用 \eqref{3.44}，其中参数 $\lambda$ 可能不同，或者采用相同的 $\lambda$．第一种策略允许对不同的输出应用不同程度的正则化，但是需要估计 $k$ 个独立的正则化参数 $\lambda_1,\ldots,\lambda_k$，而第二种策略可以在估计单独的正则化参数 $\lambda$ 时应用全部的 $k$ 个输出．

其它更复杂的收缩和选择的策略利用了多重输出情形中不同响应变量间的相关性，这可能会有帮助．举个例子，假设在这些输出变量中，我们有 

$$
\begin{align*}
Y_k & = f(X) + \varepsilon_k\tag{3.65}\label{3.65}\\
Y_\ell &= f(X) + \varepsilon_\ell\,;\tag{3.66}\label{3.66}
\end{align*}
$$

也就是说，$\eqref{3.65}$ 和 $\eqref{3.66}$ 在它们的模型中共享相同的结构 $f(X)$．这种情形下很明显我们应该合并 $Y_k$ 和 $Y_\ell$ 来估计共同的 $f$．

合并响应变量是 **典则相关分析 (canonical correlation analysis, CCA)** 的核心，这是一种为多元输出情形提出的数据降维的技巧．类似 PCA，CCA 寻找 $\x_j$ 的一列不相关的线性组合 $\X v_m, m=1,\ldots,M$，以及对应的响应变量 $\y_k$ 的不相关的线性组合 $\Y u_m$，使得相关系数

$$
\Corr^2(\Y u_m,\X v_m)\tag{3.67}\label{3.67}
$$

相继最大化．注意到最多可以找到 $M=\min(k,p)$ 个方向．第一典则响应变量是被 $\x_j$ 最优预测的线性组合（导出的响应变量）；相反地，最后的典则响应变量是被 $\x_j$ 估计最差的，并且可以考虑舍弃．CCA 的解通过对样本交叉协方差矩阵 $\Y^T\X/N$ 进行广义的 SVD 分解得到（假设 $\Y$ 和 $\X$ 均被中心化；[练习 3.20](https://github.com/szcf-weiya/ESL-CN/issues/172)）

!!! info "weiya 注：Ex. 3.20"
    已解决，详见 [Issue 172: Ex. 3.20](https://github.com/szcf-weiya/ESL-CN/issues/172)，欢迎讨论交流！

**降秩回归 (reduced-rank regression)** (Izenman, 1975[^1]; van der Merwe and Zidek, 1980[^2]) 采用显式地合并信息的回归模型来 *正式化 (formalize)* 这个方法．给定误差协方差 $\Cov(\varepsilon)=\bSigma$，求解下列带约束的多元回归问题：

$$
\hat\B^{rr}(m) = \underset{\rank(\B)=m}{\argmin}\sum_{i=1}^N(y_i-\B^Tx_i)^T\bSigma^{-1}(y_i-\B^Tx_i)\,.\tag{3.68}\label{3.68}
$$

将 $\bSigma$ 用估计值 $\Y^T\Y/N$ 替换，可以证明（[练习 3.21](https://github.com/szcf-weiya/ESL-CN/issues/174)）解由 $\Y$ 和 $\X$ 的 CCA 给出：

$$
\hat\B^{rr}(m)=\hat\B\U_m\U_m^-\,,\tag{3.69}\label{3.69}
$$

其中 $\U_m$ 是 $\U$ 中由前 $m$ 列构成的 $K\times m$ 的子矩阵，$\U$ 是 $K\times M$ 的左典则向量 $u_1,u_2,\ldots,u_M$ 构成的矩阵．$\U_m^-$ 是其广义逆．

!!! info "weiya 注：Ex. 3.21"
    已解决，详见 [Issue 174: Ex. 3.21](https://github.com/szcf-weiya/ESL-CN/issues/174)，欢迎讨论交流！

将解改写成

$$
\hat\B^{rr}(M) = (\X^T\X)^{-1}\X^T(\Y\U_m)\U_m^-\,, \tag{3.70}
$$

我们看到降秩回归在合并的响应矩阵 $\Y\U_m$ 上进行线性回归，然后将系数映射回原来的响应变量空间中（因此能拟合得很好）．降秩拟合由下式给出：

$$
\begin{align*}
\hat\Y^{rr}(m) &= \X(\X^T\X)^{-1}\X^T\Y\U_m\U_m^-\notag\\
&=\H\Y\P_m\,,\tag{3.71}
\end{align*}
$$

其中 $\H$ 是一般的线性回归映射算子，而 $\P_m$ 是秩为 $m$ 的 CCA 响应变量投影算子．尽管 $\bSigma$ 更好的估计为 $(\Y-\X\hat\B)^T(\Y-\X\hat\B)/(N-pK)$，但是可以证明解是一样的（[练习 3.22](https://github.com/szcf-weiya/ESL-CN/issues/175)）．

!!! info "weiya 注：Ex. 3.22"
    已解决，详见 [Issue 175: Ex. 3.22](https://github.com/szcf-weiya/ESL-CN/issues/175)，欢迎交流讨论！

降秩回归通过截断 CCA 从响应变量中借来了优势变量．Breiman and Friedman (1997)[^3] 探索了 $\X$ 和 $\Y$ 间典则变量的逐步收缩，这是光滑版本的降秩回归．他们的方法有如下形式（与 \eqref{3.69} 对应）

$$
\hat\B^{c+w} = \hat\B\U\bLambda\U^{-1}\,,\tag{3.72}
$$

其中 $\bLambda$ 是对角收缩矩阵（“c+w” 表示 “Curds and Whey”，这是作者取的名字）．基于总体设定中的最优预测，他们证明了 $\bLambda$ 的对角元为

$$
\lambda_m = \frac{c_m^2}{c_m^2+\frac pN(1-c_m^2)}\,,\; m = 1,\ldots,M\,,\tag{3.73}
$$

其中 $c_m$ 是第 $m$ 个典则相关系数．注意到随着输入变量个数与样本大小的比率 $p/N$ 变小，收缩因子趋向于 1．Breiman and Friedman (1997)[^3] 基于训练数据和交叉验证提出了修改版本的 $\bLambda$，但是一般形式是相同的．这里拟合响应形式为 

$$
\hat\Y^{c+w} = \H\Y\S^{c+w}\,,\tag{3.74}
$$

这里 $\S^{c+w} = \U\bLambda\U^{-1}$ 是响应变量收缩算子．

Breiman and Friedman (1997)[^3] 还建议同时在 $Y$ 和 $X$ 的空间中进行收缩．这导出混合收缩模型：

$$
\hat\Y^{\mathrm{ridge},c+w}=\A_\lambda\Y\S^{c+w}\,,\tag{3.75}
$$

其中 $\A_\lambda = \X(\X^T\X+\lambda\I)^{-1}\X^T$ 是岭回归收缩算子，和 \eqref{3.46} 一样．他们的文章及其讨论有更多细节．

!!! note "Recall"
    $$
    \begin{align}
    \mathbf{X}\hat{\beta}^{ls}&=\mathbf{X(X^TX)^{-1}X^Ty}\notag\\
    &=\mathbf{UU^Ty}\tag{3.46}\label{3.46}
    \end{align}
    $$

[^1]: Izenman, A. (1975). Reduced-rank regression for the multivariate linear model, Journal of Multivariate Analysis 5: 248–264.
[^2]: van der Merwe, A. and Zidek, J. (1980). Multivariate regression analysis and canonical variates, The Canadian Journal of Statistics 8: 27–39.
[^3]: Breiman, L. and Friedman, J. (1997). Predicting multivariate responses in multiple linear regression (with discussion), Journal of the Royal Statistical Society Series B. 59: 3–37.