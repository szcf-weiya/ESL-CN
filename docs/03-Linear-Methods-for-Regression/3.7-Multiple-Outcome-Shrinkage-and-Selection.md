# 3.7 多重输出的收缩和选择

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-10-21:2016-10-21                    |

如 3.2.4 节所述，多重输出线性模型的最小二乘估计简单地可以看成是关于每个输出的最小二乘估计。

$\eqref{3.67}$
为了在多重输出情形中应用选择和收缩的方法，可以对每个输出变量单独地应用单变量的技巧，或对全部的输出变量同时进行。举个例子，对于岭回归，我们可以对输出矩阵 $Y$ 的 $K$ 列中的每一列应用 (3.44)，其中参数 $\lambda$ 可能不同，或者采用相同的 $\lambda$。第一种策略允许对不同的输出应用不同程度的正则化，但是需要估计 $k$ 个独立的正则化参数 $\lambda_1,\ldots,\lambda_k$，而第二种策略可以在估计单独的正则化参数 $\lambda$ 时应用全部的 $k$ 个输出。

其它更复杂的收缩和选择的策略利用了多重输出情形中不同响应变量间的相关性，这可能会有帮助。举个例子，假设在这些输出变量中，我们有 

$$
\begin{align*}
Y_k & = f(X) + \varepsilon_k\tag{3.65}\label{3.65}\\
Y_\ell &= f(X) + \varepsilon_\ell\,;\tag{3.66}\label{3.66}
\end{align*}
$$

也就是说，$\eqref{3.65}$ 和 $\eqref{3.66}$ 在它们的模型中共享相同的结构 $f(X)$。这种情形下很明显我们应该合并 $Y_k$ 和 $Y_\ell$ 来估计共同的 $f$。

合并响应变量是 **典则相关分析 (canonical correlation analysis, CCA)** 的核心，这是一种为多元输出情形提出的数据降维的技巧。类似 PCA，CCA 寻找 $\x_j$ 的一列不相关的线性组合 $\X v_m, m=1,\ldots,M$，以及对应的响应变量 $\y_k$ 的不相关的线性组合 $\Y u_m$，使得相关系数

$$
\Corr^2(\Y u_m,\X v_m)\tag{3.67}\label{3.67}
$$

相继最大化。注意到最多可以找到 $M=\min(k,p)$ 个方向。第一典则响应变量是被 $\x_j$ 最优预测的线性组合（导出的响应变量）；相反地，最后的典则响应变量是被 $\x_j$ 估计最差的，并且可以考虑被舍弃。CCA 的解通过对样本交叉协方差矩阵 $\Y^T\X/N$ 进行广义的 SVD 分解得到（假设 $\Y$ 和 $\X$ 均被中心化；[练习 3.20](https://github.com/szcf-weiya/ESL-CN/issues/172)）

!!! info "weiya 注：Ex. 3.20"
    已解决，详见 [Issue 172: Ex. 3.20](https://github.com/szcf-weiya/ESL-CN/issues/172)，欢迎讨论交流！

**降秩回归 (reduced-rank regression)** (Izenman, 1975[^1]; van der Merwe and Zidek, 1980[^2]) 通过明确地合并信息的回归模型来形式化这个方法。给定误差协方差 $\Cov(\varepsilon)=\bSigma$，求解下列带约束的多元回归问题：

$$
\hat\B^{rr}(m) = \underset{\rank(\B)=m}{\argmin}\sum_{i=1}^N(y_i-\B^Tx_i)^T\bSigma^{-1}(y_i-\B^Tx_i)\,.\tag{3.68}\label{3.68}
$$

将 $\bSigma$ 用估计值 $\Y^T\Y/N$ 替换，可以证明（[练习 3.21](https://github.com/szcf-weiya/ESL-CN/issues/174)）解由 $\Y$ 和 $\X$ 的 CCA 给出：

$$
\hat\B^{rr}(m)=\hat\B\U_m\U_m^-\,,\tag{3.69}\label{3.69}
$$

其中 $\U_m$ 是 $\U$ 中由前 $m$ 列构成的 $K\times m$ 的子矩阵，$\U$ 是 $K\times M$ 的左典则向量 $u_1,u_2,\ldots,u_M$ 构成的矩阵。$\U_m^-$ 是其广义逆。

!!! info "weiya 注：Ex. 3.21"
    已解决，详见 [Issue 174: Ex. 3.21](https://github.com/szcf-weiya/ESL-CN/issues/174)，欢迎讨论交流！

将解改写成

$$
\hat\B^{rr}(M) = (\X^T\X)^{-1}\X^T(\Y\U_m)\U_m^-\,, \tag{3.70}
$$

我们看到降秩回归在合并的响应矩阵 $\Y\U_m$ 上进行线性回归，然后将系数映射回原来的响应变量空间中（因此能拟合得很好）。降秩拟合由下式给出：

$$
\begin{aligned}
\hat\Y^{rr}(m) &= \X(\X^T\X)^{-1}\X^T\Y\U_m\U_m^-\notag\\
&=\H\Y\P_m\,,\tag{3.71}
\end{aligned}
$$

其中 $\H$ 是一般的线性回归映射算子，而 $\P_m$ 是秩为 $m$ 的 CCA 响应变量投影算子。尽管 $\bSigma$ 更好的估计为 $(\Y-\X\hat\B)^T(\Y-\X\hat\B)/(N-pK)$，但是可以证明解是一样的（[练习 3.22](https://github.com/szcf-weiya/ESL-CN/issues/175)）。

!!! info "weiya 注：Ex. 3.22"
    已解决，详见 [Issue 175: Ex. 3.22](https://github.com/szcf-weiya/ESL-CN/issues/175)，欢迎交流讨论！

[^1]: Izenman, A. (1975). Reduced-rank regression for the multivariate linear model, Journal of Multivariate Analysis 5: 248–264.
[^2]: van der Merwe, A. and Zidek, J. (1980). Multivariate regression analysis and canonical variates, The Canadian Journal of Statistics 8: 27–39.