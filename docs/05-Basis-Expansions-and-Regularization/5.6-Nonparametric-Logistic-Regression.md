# 5.6 非参逻辑斯蒂回归

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 发布 | 2017-09-14 |
| 更新 | 2018-03-26, 2018-07-10|
| 状态 | Done|

[5.4 节](../5.4-Smoothing-Splines/index.html)的光滑样条问题 (5.9) 是在回归的背景下提出来的．可以非常直接地将其推广到其他领域．这里我们考虑单个定量输入变量 $X$ 的逻辑斯蒂回归．模型为

$$
\log\frac{\Pr(Y=1\mid X=x)}{\Pr(Y=0\mid X=x)}=f(x)\tag{5.28}
$$

这表明

$$
\Pr(Y=1\mid X=x)=\frac{e^{f(x)}}{1+e^{f(x)}}\tag{5.29}
$$

以光滑化的方式对 $f(x)$ 进行拟合会得到条件概率 $\Pr(Y=1\mid x)$ 的光滑估计，它可以用来分类或者风险评分．

我们构造如下的带惩罚的对数似然准则

$$
\begin{align}
\ell(f;\lambda)&=\sum\limits_{i=1}^N[y_i\LOG p(x_i)+(1-y_i)\mathrm{log}(1-p(x_i))]-\frac{1}{2}\lambda\int \{f''(t)\}^2dt\notag\\
&=\sum\limits_{i=1}^N[y_if(x_i)-\log(1+e^{f(x_i)})]-\frac{1}{2}\lambda \int\{f''(t)\}^2dt\tag{5.30}
\end{align}
$$

其中 $p(x)=\Pr(Y=1\mid x)$．上述表达式的第一项为基于二项分布的对数似然．在 [5.4 节](../5.4-Smoothing-Splines/index.html)中使用的类似的参数证明了最优的 $f$ 是结点在无重复的 $x$ 处的有限维自然样条．这意味着我们可以表示成 $f(x)=\sum_{j=1}^NN_j(x)\theta_j$．我们比较一阶微分和二阶微分

$$
\begin{align}
\frac{\partial \ell(\theta)}{\partial \theta}&=\mathbf {N^T(y-p)-\lambda\Omega}\theta\tag{5.31}\\
\frac{\partial^2\ell(\theta)}{\partial\theta\partial\theta^T}&=-\mathbf{N^TWN-\lambda \Omega}\tag{5.32}
\end{align}
$$

其中 $\mathbf p$ 是元素为 $p(x_i)$ 的 $N$ 维向量，$\mathbf W$ 是权重为 $p(x_i)(1-p(x_i)$ 的对角矩阵．(5.31) 的一阶微分关于 $\theta$ 是非线性的，所以我们需要运用和 [4.4.1 节](../04-Linear-Methods-for-Classification/4.4-Logistic-Regression/index.html)一样的迭代算法．采用类似用于线性逻辑斯蒂回归的 (4.23) 和 (4.26) 的 Newton-Raphson 算法，更新的等式可以写成

!!! note "weiya 注：Recall"
    $$
    \beta^{new}=\beta^{old}-(\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T})^{-1}\dfrac{\partial \ell(\beta)}{\partial(\beta)}\tag{4.23}
    $$
    $$
    \begin{align}
    \beta^{new}&=\beta^{old}+\mathbf{(X^TWX)^{-1}X^T(y-p)}\notag\\
    &=\mathbf {(X^TWX)^{-1}X^TW(X\beta^{old}+W^{-1}(y-p))}\notag\\
    &=\mathbf{(X^TWX)^{-1}X^TWz}\tag{4.26}
    \end{align}
    $$

$$
\begin{align}
\theta^{new} &=\mathbf{(N^TWN+\lambda\Omega)^{-1}N^TW(N\theta^{old}+W^{-1}(y-p))}\notag\\
&= \mathbf{(N^TWN+\lambda\Omega)^{-1}N^TWz}\tag{5.33}
\end{align}
$$

也可以用拟合值表示更新

$$
\begin{array}{ll}
\mathbf f^{new} &=\mathbf{N(N^TWN+\lambda\Omega)^{-1}N^TW(f^{old}+W^{-1}(y-p))}\\
&= \mathbf{S_{\lambda,w}z}\tag{5.34}
\end{array}
$$

参考 (5.12) 和 (5.14) 式，

!!! note "weiya 注：Recall"
    $$
    \hat\theta = (\mathbf N^T\mathbf N+\lambda\mathbf\Omega_N)^{-1}\mathbf N^T\mathbf y\tag{5.12}
    $$
    $$
    \begin{align}
    \hat{\mathbf f} &= \mathbf N(\mathbf N^T\mathbf N + \lambda\mathbf \Omega_N)^{-1}\mathbf N^T\mathbf y\notag\\
	            &= \mathbf S_\lambda\mathbf y\qquad\qquad\qquad\tag{5.14}
    \end{align}
    $$

我们看到更新的操作对**工作响应变量 (working response)** $\mathbf z$ 拟合了加权光滑样条（[练习 5.12](https://github.com/szcf-weiya/ESL-CN/issues/107)）．

!!! info "weiya 注：Ex. 5.12"
    已解决，详见 [Issue 107: Ex. 5.12](https://github.com/szcf-weiya/ESL-CN/issues/107)．

式 (5.34) 的形式很有指导意义．它试图用任何非参（加权）回归算子代替 $\mathbf S_{\lambda,w}$，并且得到一般的非参逻辑斯蒂回归模型的族．尽管这里 $x$ 是一维的，但这个过程可以很自然地推广到高维的 $x$．这些拓展是 **广义可加模型 (generalized additive models)** 的核心，我们将在[第 9 章](../09-Additive-Models-Trees-and-Related-Methods/9.0-Introduction/index.html)中讨论．
