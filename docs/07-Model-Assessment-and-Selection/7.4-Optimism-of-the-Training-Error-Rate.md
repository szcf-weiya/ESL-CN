# 7.4 训练误差率的optimism

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=247) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 发布 | 2016-09-30 |
|更新|2019-07-27 12:07:49|
|状态|Done|

讨论误差率的估计可能令人困惑，因为我们必须弄清楚哪些量是固定的哪些量是随机的．在我们继续之前，我们需要一些定义，详细阐述第 [7.2 节](7.2-Bias-Variance-and-Model-Complexity/index.html)的内容．给出训练集 $\cal{T}=\\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\\}$，模型 $\hat f$ 的泛化误差为

$$
\Err_{\cal T} = \mathrm E_{X^0,Y^0}[L(Y^0,\hat f(X^0))\mid {\cal T}]\tag{7.15}\label{7.15}
$$

注意到在表达式 $\eqref{7.15}$ 中训练集 $\cal T$ 是固定的．点 $(X^0,Y^0)$ 是新的测试点，从数据的联合分布 $F$ 中取的．对数据集平均得到期望误差

$$
\mathrm{Err} = \mathrm E_{\cal T}\mathrm E_{X^0,Y^0}[L(Y^0,\hat f(X^0))\mid {\cal T}]\tag{7.16}\label{7.16}
$$

它更适合于统计分析．正如之前提到的，事实证明大部分方法能够有效估计期望误差而非 $\Err_{\cal T}$；这一点详见 [7.12 节](7.12-Conditional-or-Expected-Test-Error/index.html)．

!!! note "weiya 注："
    这里原文为 $\E_{\cal T}$，但结合上下文此处应为 $\Err_{\cal T}$.

现在一般地，训练误差

$$
\overline{\err} = \frac{1}{N}\sum\limits_{i=1}^NL(y_i,\hat f(x_i))\tag{7.17}\label{7.17}
$$

会比真实误差 $\Err\_{\cal T}$ 小，因为数据被同时用来拟合方法并且评估误差（见[练习 2.9](https://github.com/szcf-weiya/ESL-CN/issues/176)）．拟合方法一般适应于训练数据，因此训练误差 $\overline{\err}$ 是对泛化误差 $\mathrm {Err}\_{\cal T}$过度的乐观估计．

!!! note "weiya 注：Ex. 2.9"
    已解决，详见 [Issue 176: Ex. 2.9](https://github.com/szcf-weiya/ESL-CN/issues/176).

部分差异是因为取值点的选取．值 $\Err_{\cal T}$ 可以看成是 **样本外 (extra-sample)** 误差，因为测试输入向量不需要与训练输入向量一致．当我们去关注 **样本内 (in-sample)** 误差，可以很简单地理解 $\overline{\err}$ 乐观估计的本质

$$
\Err_{in}=\frac{1}{N}\sum\limits_{i=1}^N\mathrm E_{Y^0}[L(Y_i^0,\hat f(x_i))\mid {\cal T}]\tag{7.18}\label{7.18}
$$

$Y^0$ 表示我们在每个训练点 $x_i,i=1,2,\ldots,N$ 处观测 $N$ 个新响应变量的值．我们定义 $\Err_{in}$ 与训练误差 $\overline{\err}$ 的差为 **乐观 (optimism)**：

$$
\mathrm {op}\equiv \mathrm{Err}_{in}-\overline{\err}\tag{7.19}\label{7.19}
$$

一般情形下这是正的，因为 $\overline{\err}$ 经常是预测误差的向下有偏估计．最终，平均乐观是乐观在训练集上的期望

$$
\omega \equiv E_{\mathbf y}(\mathrm{op})\tag{7.20}\label{7.20}
$$

这里训练集中的预测变量是固定的，并且期望是对训练集的输出值而言的；因此我们已经用记号 $\E_{\mathbf y}$ 而不是 $\E_{\cal T}$．我们通常仅仅估计期望误差 $\omega$ 而不是 op，就像我们可以估计期望误差 $\Err$ 而不是条件误差 $\Err_{\cal T}$．

对于平方误差，0-1，以及其他损失函数，可以证明一般性的结论

$$
\omega=\frac{2}{N}\sum\limits_{i=1}^N\Cov(\hat y_i,y_i)\tag{7.21}\label{7.21}
$$

其中 $\Cov$ 表示协方差．因此 $\overline{\err}$ 低估真实误差的程度取决于 $y_i$ 影响它本身估计的程度．数据越难拟合，$\Cov(\hat y_i,y_i)$ 将越大，因此增大了乐观．[练习 7.4](https://github.com/szcf-weiya/ESL-CN/issues/27) 证明了平方损失函数时的结果，其中 $\hat y_i$ 为根据回归拟合好的值．对于 0-1 损失，$\hat y_i\in\\{0,1\\}$ 是在 $x_i$ 处的分类，另外对于熵损失，$\hat y_i\in[0,1]$ 是在 $x_i$ 处类别 1 的拟合概率．

!!! note "weiya 注：Ex. 7.4"
    已解决，详见 [Issue 27: Ex. 7.4](https://github.com/szcf-weiya/ESL-CN/issues/27).

总结一下，我们有重要的关系式

$$
\E_{\mathbf y}(\Err_{in})=\E_{\mathbf y}(\overline{\err})+\frac{2}{N}\sum\limits_{i=1}^N\Cov(\hat y_i,y_i)\tag{7.22}\label{7.22}
$$

如果 $\hat y_i$ 通过含 $d$ 个输入或者基函数的线性拟合得到，上面表达式可以简化．例如，对于可加误差模型 $Y=f(X)+\varepsilon$，

$$
\sum\limits_{i=1}^N\Cov(\hat y_i,y_i) = d\sigma_{\varepsilon}^2\tag{7.23}\label{7.23}
$$

因此

$$
\E_{\mathbf y}(\Err_{in})=\E_{\mathbf y}(\overline{\err})+2\cdot\frac{d}{N}\sigma_\varepsilon^2\tag{7.24}\label{7.24}
$$

!!! note "weiya 注：\eqref{7.24}"
    [Issue 27: Ex. 7.4](https://github.com/szcf-weiya/ESL-CN/issues/27) 的解答中也给出了 \eqref{7.24} 的推导。

表达式 $\eqref{7.23}$ 是将在 [7.6 节](7.6-The-Effective-Number-of-Parameters/index.html)讨论的 **有效参数个数 (effective number of parameters)** 定义的基础．optimism 随着我们使用的输入或基函数的个数 $d$ 线性增长，但是当训练样本大小增大时会降低．$\eqref{7.24}$ 的其它版本对其它误差模型也近似成立，比如二值数据和熵损失．

估计预测误差的一种明显方法是先估计 optimism 然后加到训练误差 $\overline{\err}$ 上．下一节将要描述的方法—— $C_p$，AIC，BIC 以及其它方法——对于估计关于参数是线性的特殊估计类，都是通过这种方式实现．

相反地，将在本章后面描述的交叉验证以及自助法是对 **样本外 (extra-sample)** 误差 $\Err$ 直接估计的方法．这些一般工具可以用于任意损失函数以及非线性自适应拟合技巧．

样本内误差通常不是直接感兴趣的，因为特征的未来值不可能与它们训练集值一致．但是为了模型之间的比较，样本内误差是很方便的，并且经常能够有效地进行模型选择．原因在于误差的相对（而不是绝对）大小是我们所关心的．
