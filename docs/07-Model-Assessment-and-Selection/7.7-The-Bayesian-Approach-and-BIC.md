# 7.7 贝叶斯方法和BIC

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 发布 | 2016-09-30 |
| 更新 |2019-07-27 18:23:22|
|状态|Done|

和 AIC 一样，**贝叶斯信息准则 (BIC)** 在由极大似然得到的拟合的设定中是可行的．BIC 一般形式为

$$
\mathrm{BIC} = -2\cdot \loglik +(\log N)\cdot d\tag{7.35}\label{7.35}
$$

BIC 准则（乘以 1/2）也被称作 Schwarz 准则 (Schwarz，1978[^1])．

高斯模型下，假设方差 $\sigma_\varepsilon^2$ 是已知的，$-2\cdot \loglik$ 在忽略常数意义下等于 $\sum_i(y_i-\hat f(x_i))^2/(\sigma_\varepsilon^2)$，对于平方误差损失等于 $N\cdot\overline{\err}/\sigma_\varepsilon^2$．因此我们可以写成
$$
\mathrm{BIC} = \frac{N}{\sigma_\varepsilon^2}[\overline{\err}+(\log N)\cdot\frac{d}{N}\sigma_\varepsilon^2]\tag{7.36}
$$
因此当用 2 替换 $\log N$ 后，BIC 与 AIC（$C_p$）成比例的．假设 $N > e^2\approx 7.4$，BIC 趋向于对复杂模型惩罚更重，偏向于选择更简单的模型．如同 AIC，$\sigma_\varepsilon^2$ 一般通过低偏差模型的均方误差来估计．

!!! note "weiya 注："
    注意此处假设 $\sigma_\varepsilon^2$ 已知，如果将其看成未知，并通过 MLE 求解，则 BIC 准则（忽略常数）为
    $$
    \mathrm{BIC} = N\log\mathrm{RSS} + d\log N\,,
    $$
    详见 [Wiki: BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Gaussian_special_case)

对于分类问题，选择交叉熵作为误差衡量，使用多项对数似然会导出与 AIC 更相似的关系．注意到误分类误差率不会出现在 BIC  中，因为它并不对应于在任何概率模型下数据的对数似然．

尽管和 AIC 很相似，但 BIC 的来源 (motivation) 截然不同．它来源于使用贝叶斯方法来选择模型，我们现在进行讨论．

假设我们有一系列预选模型 ${\cal M_m},m=1,\ldots,M$，以及对应的模型参数 $\theta_m$，我们希望从中选择最优的模型．假设我们对每个模型 $\cal M_m$ 的参数有先验分布 $\Pr(\theta_m\mid\cal M_m)$，则模型的后验概率为
$$
\begin{align}
\Pr({\cal M}_m\mid \mathbf Z) &\varpropto \Pr({\cal M_m})\cdot \Pr(\mathbf Z\mid {\cal M_m})\qquad\tag{7.37}\label{7.37}\\
&\varpropto \Pr({\cal M_m})\cdot \int \Pr(\mathbf Z\mid \theta_m,{\cal M_m})\Pr(\theta_m\mid\cal M_m)d\theta_m\notag
\end{align}
$$
其中 $\mathbf Z$ 表示训练数据 $\\{x_i,y_i\\}\_1^N$．为了比较两个模型 $\cal M_m$ 和 $\cal M_\ell$，我们构造后验 odds
$$
\frac{\Pr({\cal M_m}\mid\mathbf Z)}{\Pr({\cal M_\ell\mid \mathbf Z})}=\frac{\Pr(\cal M_m)}{\Pr(\cal M_\ell)}\cdot\frac{\Pr(\mathbf Z\mid \cal M_m)}{\Pr(\mathbf Z\mid \cal M_\ell)}\tag{7.38}
$$
如果 odds 大于 1，则我们选择模型 $m$，否则我们选择模型 $\ell$．最右端的值
$$
\mathrm{BF}(\mathbf Z)=\frac{\Pr(\mathbf Z\mid\cal M_m)}{\Pr(\mathbf Z\mid \cal M_\ell)}\tag{7.39}
$$
称为 **贝叶斯因子 (Bayes factor)**, 这是数据对于后验 odds 的贡献．

一般地我们假设模型的先验分布是均匀的，所以 $\Pr(\cal M_m)$ 为常值．我们需要其它的方式来估计 $\Pr(\mathbf Z\mid \cal M_m)$．在某些简化 (Ripley, 1996[^2]) 下，对式 \eqref{7.37} 采用被称作对积分的 Laplace 近似得到
$$
\log \Pr(\mathbf Z\mid {\cal M_m})=\mathrm{log} \Pr(\mathbf Z\mid \hat \theta_m,{\cal M_m})-\frac{d_m}{2}\cdot \mathrm{log} N + O(1)\tag{7.40}
$$
这里 $\hat \theta_m$ 为极大似然估计，且$ d_m$ 为模型 $\cal M_m$ 自由参数的个数．如果我们定义我们的损失函数为
$$
-2\log \Pr(\mathbf Z\mid \hat\theta_m,\cal M_m)
$$
这等价于 \eqref{7.35} 的 BIC 准则．

因此，选择 BIC 最小的模型等价于选择有最大（近似）后验概率的模型．但是这个框架可以给我们更多的信息．如果我们对 $M$ 个元素的模型集合进行计算 BIC 准则，得到 $\mathrm{BIC}_m,m=1,2,\ldots,M$，则我们可以估计每个模型 $\cal M_m$ 的后验概率为
$$
\frac{e^{-\frac{1}{2}\cdot \mathrm{BIC}_m}}{\sum_{\ell=1}^Me^{-\frac{1}{2}\cdot \mathrm{BIC}_\ell}}\tag{7.41}
$$
因此我们不仅可以估计最优的模型，还可以所考虑的模型的相对表现．

用于模型选择，AIC 和 BIC 之间没有明显的选择．BIC 作为选择准则是渐近一致的，这意味着给出模型族，其中包含真实模型，BIC 选择正确模型的概率当 $N\rightarrow \infty$ 时是 1．对于 AIC 这是不成立的，当 $N\rightarrow \infty$ 时，趋向于选择太复杂的模型．另一方面，在有限样本时，BIC 经常选择太过简单的模型，因为对模型复杂度的惩罚更大．

!!! note "weiya 注"
    BIC 是强相合的，而 AIC 不是．
    **强相合估计：** 如果 $\hat\theta_N\; a.s.$ 收敛到 $\theta$，则称 $\hat\theta_N$ 是 $\theta$ 的强相合估计．

[^1]: Schwarz, G. (1978). Estimating the dimension of a model, Annals of Statistics 6(2): 461–464.
[^2]: Ripley, B. D. (1996). Pattern Recognition and Neural Networks, Cambridge University Press.
