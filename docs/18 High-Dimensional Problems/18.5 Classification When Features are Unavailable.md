# 当特征不可用时的分类

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-08-10                    |

!!! note "proximity matrix"
    关系矩阵，近似性矩阵，样品间的相似性矩阵。

现实中很多研究对象都很抽象，定义一个特征向量不是很显然的。但是只要我们能够得到$N\times N$的相似性矩阵，我们就可以将相似性解释为内积，这样许多分类器都可以用上去了。

## 例子：字符串核和蛋白质分类

本节主要讲衡量成对蛋白质模块之间的相似性。

为了构造特征，统计字符串中给定长度为$m$的序列出现的次数，并且计算长度为$m$的所以可能序列的个数。正式的表述是，对于字符串$x$，我们定义特征映射

$$
\Phi_m(x)=\{\phi_a\}_{a\in\cal A_m}\qquad (18.25)
$$

其中$\cal A_m$ 是长度为$m$的子序列的集合，而$\phi_a(x)$为“a”出现在字符串$x$出现的次数。应用这个，我们定义内积

$$
K_m(x_1,x_2)=\langle \Phi_m(x_1),\Phi_m(x_2) \rangle\qquad (18.26)
$$

它衡量了两个字符串$x_1$和$x_2$的相似性。举个例子，这个用到导出用于将字符串分到不同蛋白质类中的支持向量机。

现在可能序列$a$的个数为$\cal A_m=20^m$，对于适当的$m$，这个值可以非常大，并且大部分子序列与我们训练集中的子序列并不匹配。结果是我们可以高效地运用树结构来计算$N\times N$的内积矩阵或者字符串核$\mathbf K_m$(18.26)，而不用实际计算单个向量。这个方法，以及接下来的数据，来着Leslie等人（2003）的工作。

数据中包含两个类别的1708个蛋白质——阴性（1663）和阳性（45）。下面这两个该数据集中的例子，称为$x_1$和$x_2$，这两条字符串中已经标出LQE的子序列。可能存在$20^3$的子序列，所以$\Phi_3(x)$是一个长度为8000的向量。对于这个例子，$\phi_{LQE}=1,\phi_{LQE}=2$。


!!! note "ROC curve"
    ROC是receiver operating characteristics的首字母缩写，用来同时展现分类器的specificity和sensitivity，横轴一般为specificity (或false positive rate， 其中specificity = 1 - false positive rate)，纵轴一般为sensitivity (或true positive rate，其中sensitivity = true positive rate)，通过在不同的阈值下计算这两个值，就可以得到ROC曲线。分类器的整体表现可以由ROC曲线下的面积AUC (area under the curve)给出。

使用Leslie等人（2003）的软件，我们计算$m=4$时的核，它可以用在支持向量分类器中来在$20^4=160000$维特征空间中寻找最大边缘解(maximal margin solution)。我们采用10折交叉验证来计算在所有训练数据中的SVM预测值。图18.9中的橘黄色曲线显示了支持向量分类器的ROC曲线，通过改变交叉验证的支持向量分类器的预测变量的阈值计算得到。曲线下的面积为0.84。

![](../img/18/fig18.9.png)
