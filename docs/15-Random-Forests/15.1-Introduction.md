# 15.1 导言

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 发布 | 2017-02-08 |
| 更新 | 2020-01-09 10:05:13|
| 状态 | Done|

[8.7 节](/08-Model-Inference-and-Averaging/8.7-Bagging/index.html)中的 bagging 或者 bootstrap aggregation 都是为了降低估计的预测函数的方差．Bagging 似乎对于高方差、低偏差的过程表现得特别好，比如树．对于回归， 我们简单地对训练数据的 bootstrap 版本进行多次同样的回归树，然后取结果的平均．对于分类，作为 committee 的每棵树对预测的类别进行投票．



!!! note "weiya 注"
    在 [8.8 节](/08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking/index.html)，用了这样一句话来介绍 Committee Method，“Committee methods take a simple unweighted average of the predictions from each model, essentially giving equal probability to each model.” 简单来说就是，对于一系列模型，committee 方法将这一系列模型得到的预测值进行简单平均．比如，对于分类而言，模型的预测值是类别标签，Committee 方法则是统计各个模型预测的类别标签（然后进行平均，如果只关注票数，是否平均并不重要），这就类似选举时委员会投票．

[第 10 章](/10-Boosting-and-Additive-Trees/10.1-Boosting-Methods/index.html)中的 boosting 一开始也是作为 committee 的方法提出来的，尽管不像 bagging，其中作为 committee 的 weak learners 随着时间不断进化，并且成员会投出带有权重的票．Boosting 似乎在大部分问题上表现最好，而且成为更好的选择．

**随机森林 (Random forests)** (Breiman, 2001[^1])是对 bagging 本质上的修改，而且建了一个 **去相关性(de-correlated)** 树的集合，然后进行平均．在许多问题上，随机森林的表现与 boosting 的表现很相似，而且可以更加简单地进行训练和调参．结果导致，随机森林很流行，而且可以通过各种包来实现．

[^1]: Breiman, L. (2001). Random forests, Machine Learning 45: 5–32.
