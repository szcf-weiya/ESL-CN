# 为什么是指数损失？

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-06                               |

AdaBoost.M1算法最初是从一个非常不一样的角度出发得到的，而不是上一节中提出的。它与基于指数损失的向前逐步加性建模的等价性仅在其提出后五年后才被发现。通过研究指数损失函数准则的性质，可以深入了解这个过程并且找出可能改善的方式。

在加性模型中指数损失的主要吸引点在于计算上的方便；它导出简单的模块化重加权的AdaBoost算法。然而，调查它的统计性质也是我们关注的。它估计什么，估计有多么好？第一个问题可以通过寻找总体最小化来回答。

可以很简单地证明（Friedman等人，2000）
$$
f^*(x)=\mathrm{arg}\;\underset{f(x)}{\mathrm{min}}E_{Y\mid X}(e^{-Yf(x)})=\frac{1}{2}\mathrm{log}\frac{Pr(Y=1\mid X)}{Pr(Y=-1\mid X)}\qquad (10.16)
$$
或者等价地，
$$
Pr(Y=1\mid x)=\frac{1}{1+e^{-2f^*(x)}}
$$
因此，由AdaBoost得到的加性展开是估计二分之一的$P(Y=1\mid x)$比率。这证明使用它的符号作为（10.1）的分类规则是合理的。

另外一个有着同样的总体最小化的损失准则为负二项分布或者偏差（也称为交叉熵），将$f$解释为logit变换。令
$$
p(x)=Pr(Y=1\mid x)=\frac{e^{f(x)}}{e^{-f(x)}+e^{f(x)}}=\frac{1}{1+e^{-2f(x)}}\qquad (10.17)
$$
并且定义$Y'=(Y+1)/2\in \{0,1\}$.则二项对数概率损失函数为
$$
l(Y,p(x))=Y'\mathrm{log}p(x)+(1-Y')\mathrm{log}(1-p(x))
$$
或者等价地，偏差为
$$
-l(Y,f(x))=\mathrm{log}(1+e^{-2Yf(x)})\qquad (10.18)
$$

> weiya注：
> $$
> l(Y,p(x))=Yf(x)-f(x)-log(1+e^{-2Yf(x)})
> $$
> 又
> $$
> Y=1
> $$
>

因为对数概率的总体最大在真实概率$p(x)=Pr(Y=1\mid x)$处，我们从（10.17）看到偏差$E_{Y\mid x}[-l(Y,f(x))]$和$E_{Y\mid x}[e^{-Yf(x)}]$的总体最小点是一样的。因此，使用任意一个准则在总体的水平下得到相同的解。注意到$e^{-Yf}$本身不是一个合适的对数概率，因为它不是一个二项随机变量$Y\in \{-1,1\}$的任何概率密度函数的对数。