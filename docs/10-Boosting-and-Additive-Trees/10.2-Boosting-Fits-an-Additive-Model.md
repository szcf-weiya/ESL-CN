# 10.2 Boosting 拟合可加模型

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=360) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 发布 | 2017-02-08 |
| 更新   | 2017-08-26, 2018-02-28                               |
|状态|Done|

Boosting 的成功真的不是很神秘．关键在于 (10.1) 式．

!!! note "weiya 注：Recall"
    $$
    G(x)=\mathrm {sign}(\sum\limits_{m=1}^M\alpha_mG_m(x))\tag{10.1}
    $$

Boosting 是在一组基本的“基”函数集合中拟合 **加性展开 (additive expansions)** 的方法．这里基函数是单个的分类器 $G_m(x)\in \\{-1,1\\}$．更一般地，基函数展开式采用下列形式

$$
f(x)=\sum\limits_{m=1}^M\beta_mb(x;\gamma_m)\tag{10.3}
$$

其中 $\beta_m,m=1,2,\ldots,M$ 是展开的系数，$b(x;\gamma)\in \IR$ 是普通的关于多元变量 $x$ 的简单函数，$\gamma$ 是其参数．我们在 [第 5 章](../05-Basis-Expansions-and-Regularization/5.1-Introduction/index.html) 中详细讨论了基的展开．

类似的加性展开是本书介绍的许多学习方法的核心：

- 在单层神经网络中（[第 11 章](../11-Neural-Networks/11.1-Introduction/index.html)），$b(x;\gamma)=\sigma(\gamma_0+\gamma_1^Tx)$，其中 $\sigma(t)=1/(1+e^{-t})$ 是 sigmoid 函数，并且 $\gamma$ 参量化输入变量的线性组合．
- 在信号处理中，小波（[5.9.1 节](../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing/index.html)）用 $\gamma$ 参量化“母”微波的位置和偏移量是很流行的选择．多元自适应回归样条（[9.4 节](../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS/index.html)）采用截断幂样条基函数，其中 $\gamma$ 对变量和结点的值进行参量化．
- 对于树，$\gamma$ 对中间结点的分离变量和分离点以及终止结点的预测值进行参量化．

一般地，通过使训练数据上的平均损失函数最小化来拟合这些模型，比如平方误差或者基于概率的损失函数，
$$
\underset{\{\beta_m,\gamma_m\}_1^M}{\mathrm {min}}\;\sum\limits_{i=1}^NL(y_i,\sum\limits_{m=1}^M\beta_mb(x_i;\gamma_m))\tag{10.4}
$$
对于许多损失函数 $L(y,f(x))$ 和/或基函数 $b(x;\gamma)$，这需要 **计算密集型的 (computationally intensive)** 数值优化技术．

!!! note "weiya 注：computationally intensive"
    引用[什么是计算密集型（computationally intensive）？ - 知乎](https://www.zhihu.com/question/23243435)的回答
    > 程序系统大部分在做计算、逻辑判断、循环导致 cpu 占用率很高的情况，称之为计算密集型；频繁网络传输、读取硬盘及其他 io 设备称之为 io 密集型
    

然而，当可以快速求解只有一个单一的基函数的子问题时，经常可以找到简单的替代方案，单一的基函数为
$$
\underset{\beta,\gamma}{\mathrm{min}}\sum\limits_{i=1}^NL(y_i,\beta b(x_i;\gamma))\tag{10.5}
$$
